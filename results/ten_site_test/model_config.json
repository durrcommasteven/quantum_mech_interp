{
    "n_layers": 2,
    "d_model": 32,
    "n_ctx": 11,
    "d_head": 8,
    "model_name": "custom",
    "n_heads": 4,
    "d_mlp": null,
    "act_fn": "relu",
    "d_vocab": 3,
    "eps": 1e-05,
    "use_attn_result": false,
    "use_attn_scale": true,
    "attn_scale": 2.8284271247461903,
    "use_split_qkv_input": false,
    "use_hook_mlp_in": false,
    "use_attn_in": false,
    "use_local_attn": false,
    "original_architecture": null,
    "from_checkpoint": false,
    "checkpoint_index": null,
    "checkpoint_label_type": null,
    "checkpoint_value": null,
    "tokenizer_name": null,
    "window_size": null,
    "attn_types": null,
    "init_mode": "gpt2",
    "normalization_type": "LN",
    "device": "cpu",
    "n_devices": 1,
    "attention_dir": "causal",
    "attn_only": true,
    "seed": null,
    "initializer_range": 0.1414213562373095,
    "init_weights": true,
    "scale_attn_by_inverse_layer_idx": false,
    "positional_embedding_type": "standard",
    "final_rms": false,
    "d_vocab_out": 1,
    "parallel_attn_mlp": false,
    "rotary_dim": null,
    "n_params": 8192,
    "use_hook_tokens": false,
    "gated_mlp": false,
    "default_prepend_bos": true,
    "dtype": 